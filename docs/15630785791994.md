# PV，PVC 体系的作用，从本地持久化卷谈起

本地持久化存储。

不过，首先需要明确的是，Local Persistent Volume 并不适用于所有应用。事实上，它的适用范围非常固定，比如：高优先级的系统应用，需要在多个不同节点上存储数据，并且对 I/O 较为敏感。典型的应用包括：分布式数据存储比如 MongoDB、Cassandra 等，分布式文件系统比如 GlusterFS、Ceph 等，以及需要在本地磁盘上进行大量数据缓存的分布式应用。

其次，相比于正常的 PV，一旦这些节点宕机且不能恢复时，`Local Persistent Volume` 的数据就可能丢失。这就要求使用 Local Persistent Volume 的应用必须具备数据备份和恢复的能力，允许你把这些数据定时备份在其他位置。

## 本地持久化存储设计的难点


### 难点一
如何把本地磁盘抽象成 PV。

比如，一个 Pod 可以声明使用类型为 Local 的 PV，而这个 PV 其实就是一个 hostPath 类型的 Volume。如果这个 hostPath 对应的目录，已经在节点 A 上被事先创建好了。那么，我只需要再给这个 Pod 加上一个 nodeAffinity=nodeA，不就可以使用这个 Volume 了吗？

事实上，你**绝不应该把一个宿主机上的目录当作 PV 使用**。这是因为，这种本地目录的存储行为**完全不可控**，它所在的磁盘随时都可能被应用写满，甚至造成整个宿主机宕机。而且，不同的本地目录之间也**缺乏哪怕最基础的 I/O 隔离机制**。

所以，一个 Local Persistent Volume 对应的存储介质，一定是**一块额外挂载在宿主机的磁盘或者块设备**（“额外”的意思是，它不应该是宿主机根目录所使用的主硬盘）。这个原则，我们可以称为“**一个 PV 一块盘**”。

### 难点二
调度器如何保证 pod 始终能被正确地调度到它所请求的 Local Persistent Volume 所在的节点上？

造成这个问题的原因在于，对于常规的 PV 来说，Kubernetes 都是**先调度 Pod 到某个节点上**，然后，再**通过“两阶段处理”来“持久化”**这台机器上的 Volume 目录，进而完成 Volume 目录与容器的绑定挂载。

可是，对于 Local PV 来说，节点上可供使用的磁盘（或者块设备），**必须是运维人员提前准备好的**。它们在不同节点上的挂载情况可以完全不同，甚至有的节点可以没这种磁盘。

**所以，这时候，调度器就必须能够知道所有节点与 Local Persistent Volume 对应的磁盘的关联关系，然后根据这个信息来调度 Pod。**

这个原则，我们可以称为**“在调度的时候考虑 Volume 分布”**。在 Kubernetes 的调度器里，有一个叫作 `VolumeBindingChecker` 的过滤条件专门负责这个事情。在 Kubernetes v1.11 中，这个过滤条件已经默认开启了。

