# Linux kernel 处理数据包的过程


## 1.Linux内核中数据接受
![](https://i0.wp.com/opensourceforu.com/wp-content/uploads/2016/08/Figure-1-Data-receiving-process.jpg)
数据目的地为特定系统的数据首先由NIC接收，并存储在NIC中存在的接收（RX）的环形缓冲区中，该缓冲区还具有TX（用于数据传输）。一旦内核可以访问数据包，设备驱动程序就会引发softirq（软件中断），这会使系统的DMA（数据存储器访问）将该数据包发送到Linux内核。Linux内核中的数据包数据存储在sk_buff数据结构中，以将数据包保存到MTU（最大传输单元）。当所有数据包都填充在内核缓冲区中时，它们会被发送到上层处理层 IP，TCP或UDP。然后将数据复制到首选数据接收过程。

### 1.1The NIC ring buffer
接收环形缓冲区在设备驱动程序和NIC之间共享。网卡分配一个发射机 ( TX )和接收( RX )环形缓冲器。顾名思义，环形缓冲区是一个循环缓冲区，其中 溢出只是覆盖现有数据。应该注意，移动数据有两种方式 从网卡到内核，硬件中断和软件中断，也称为软件中断。

RX环形缓冲器用于存储传入数据包，直到设备驱动程序能够处理它们。设备驱动程序通常通过SoftIRQs来排空RX环，这将传入的数据包放入称为sk _ buffer或“skb”的内核数据结构中，开始其通过内核并到达拥有相关套接字的应用程序。TX环形缓冲器用于保存发往有线的输出数据包。

这些环形缓冲区位于堆栈的底部，是数据包丢失的关键点，这反过来会对网络性能产生不利影响。

### 1.2 Interrupts and Interrupt Handlers

来自硬件的中断被称为“top-half”中断。当NIC接收到传入数据时，它会使用DMA将数据复制到内核缓冲区中。NIC通过引发硬中断将此数据通知内核。这些中断由做最少工作的中断处理程序处理，因为它们已经中断了另一项任务，并且自身不能被中断。就CPU使用而言，硬中断可能会很昂贵，尤其是当持有内核锁时。

然后，硬中断处理程序将大部分数据包接收留给软件中断( SoftIRQ )处理，这可以更公平地安排。


在`/ proc /interrupts`中可以看到硬中断，其中每个队列都有一个中断向量分配给它的第一列。当系统启动或加载NIC设备驱动程序模块时，这些都将初始化。每个RX和TX队列都被分配一个唯一的向量，该向量通知中断处理程序中断来自哪个NIC 队列。这些列将传入中断的数量表示为计数器值
![](https://ws1.sinaimg.cn/large/006tNc79ly1fzoglrba6dj30p60g3ad5.jpg)

### 1.3 SoftIRQs

软件终端也称为“bottom-half”中断，软件中断请求( SoftIRQs )是内核例程，计划在其他任务不会中断的时候运行。SoftIRQ的目的是耗尽网络适配器接收环形缓冲区。这些例程以ksoftirqd/cpu号进程的形式运行，并调用特定于驱动程序的代码函数。它们可以在过程监控工具中看到，如ps和top。

下面的调用堆栈从下往上读取，是SoftIRQ轮询Mellanox卡的一个例子。标记为[mlx4_en]的函数是mlx4_en.ko驱动程序内核模块中的Mellanox轮询例程，由内核的通用轮询例程调用，如net_rx_action。从驱动程序移动到内核后，接收到的流量将向上移动到套接字，为应用程序使用做好准备:

![](https://ws3.sinaimg.cn/large/006tNc79ly1fzogingjvhj30i704p74o.jpg)

SoftIRQs可按如下方式进行监控。每一列代表一个CPU :

```
watch -n1 grep RX /proc/softirqs
watch -n1 grep TX /proc/softirqs
```

### 1.4NAPI Polling
NAPI (新API )的编写是为了提高处理传入卡数据包的效率。硬中断是昂贵的，因为它们不能被中断。即使有中断合并(稍后将更详细地描述)，中断处理程序也会完全独占CPU内核。NAPI的设计允许驱动程序进入轮询模式，而不是在每次收到所需数据包时都被硬中断。

在正常操作下，初始硬中断或IRQ被引发，随后是软IRQ处理器，该处理器使用NAPI例程轮询卡。轮询例程有一个预算，用于确定允许代码的CPU时间。这是防止软寄存器垄断CPU。完成后，内核将退出轮询例程并重新启动，然后整个过程将会重复。
![](https://ws3.sinaimg.cn/large/006tNc79ly1fzogvtpcuhj30da0bvq4a.jpg)

### 1.4Network Protocol Stacks

一旦流量从网卡被接收到内核中，它就会被协议处理器处理，例如以太网、ICMP、IPv4、IPv6、TCP、UDP和SCTP。

最后，数据被传递到套接字缓冲区，在那里应用程序可以运行接收功能，将数据从内核空间移动到用户空间，并结束内核对接收过程的参与。

### 1.5 Packet egress in the Linux kernel
Linux内核的另一个重要方面是网络数据包出口。虽然出口比入口逻辑简单，但仍然值得确认。当skbs从协议层向下传递到核心网络例程时，这个过程就开始了。每个skb包含一个dev字段，该字段包含将通过其传输的net_device的地址:

![](https://ws2.sinaimg.cn/large/006tNc79ly1fzoh1oqep7j30ig025jri.jpg)

它使用此字段将skb路由到正确的设备:
![](https://ws4.sinaimg.cn/large/006tNc79ly1fzoh1qrqm1j30i200k3yf.jpg)

基于此设备，执行将切换到驱动程序例程，该例程处理skb，并最终将数据复制到NIC，然后on the wire。这里需要的主要调整是TX排队规程( qdisc )队列，稍后将对此进行描述。一些网卡可以有多个TX队列。

以下是取自测试系统的堆栈跟踪示例。在这种情况下，流量通过环回设备，但这可以是任何NIC模块:
![](https://ws4.sinaimg.cn/large/006tNc79ly1fzoh36e1haj30hx088tas.jpg)

## 2 Networking Tools

**netstat**
命令行实用程序，可以打印有关开放网络连接和协议栈统计信息。它从/proc/net/文件系统中检索有关网络子系统的信息。这些文件包括:
- **/proc/net/dev**
- **/proc/net/tcp**
- **/proc/net/udp**
- **/proc/net/unix**

更多的使用方法参考 man netstat

**dropwatch**
监控实用程序，用于监控内核从内存释放的数据包。有关更多信息，请参阅dropboard手册页: man dropboard。

**ethtool**
显示和更改网卡设置的实用程序。有关详细信息，请参阅 man ethtool

**/proc/net/snmp**

显示snmp代理的IP、ICMP、TCP和UDP管理信息库所需的ASCII数据的文件。它还显示实时UDP-lite统计信息。

ifconfig命令使用旧式IOCTLs从内核中检索信息。与使用内核Netlink接口的ip命令相比，这种方法已经过时。使用ifconfig命令调查网络流量统计数据是不精确的，因为网络驱动程序不能保证这些统计数据得到一致更新。我们建议使用ip命令而不是ifconfig命令。

## 3.Persisting Tuning Parameters Across Reboots
许多网络优化设置是由sysctl程序控制的核心可调参数。sysctl程序可用于读取和更改给定参数的运行时配置。

例如，要读取TCP选择性确认可调参数，可以使用以下命令:
![](https://ws4.sinaimg.cn/large/006tNc79ly1fzohmcdnitj30iu014glk.jpg)
要更改可调参数的运行时值，还可以使用sysctl :
![](https://ws2.sinaimg.cn/large/006tNc79ly1fzohme8p97j30ii015t8o.jpg)

但是，此设置仅在当前运行时更改过，并且如果系统重新启动，将更改回内核的内置默认值。

设置保存在/etc/sysctl.conf文件中，并单独保存。具体修改方式如下：
![](https://ws1.sinaimg.cn/large/006tNc79ly1fzohpvlfp4j30iu0160so.jpg)

配置文件中指定的值将在引导时应用，之后可以使用sysctl -p命令随时重新应用。

### 3.1Identifying the bottleneck
当网卡上的RX缓冲区不能被内核足够快地耗尽时，数据包丢失和溢出通常会发生。当数据离开网络的速率超过内核排出数据包的速率时，NIC会在NIC缓冲区满后丢弃传入的数据包，并增加丢弃计数器。在ethtool统计中可以看到相应的计数器。这里的主要标准是中断和软中断，它们响应硬件中断并接收流量，然后在net.core.netdev _ budget指定的持续时间内轮询卡上的流量。在硬件级别观察数据包丢失的正确方法是使用ethtool。

确切的计数器因驱动而异；请咨询驱动程序供应商或驱动程序文档以获得适当的统计数据。一般来说，查找名称如下的计数器
失败、未命中、错误、丢弃、buf、fifo、满或丢弃。统计数据可以是大写或小写。
例如，此驱动程序会增加各种rx _ * _错误统计信息:
![](https://ws4.sinaimg.cn/large/006tNc79ly1fzohvmor3lj30hz04kmxi.jpg)

有多种工具可用于隔离问题区域。通过调查以下几点找到瓶颈:
- 适配器防火墙级别
        - 观察ethtool -S ethX 的统计数据
- 适配器驱动级别
- linux内核，IRQS或softIRQs
    - Check /proc/interrupts and /proc/net/softnet_stat
- The protocol layers IP, TCP, or UDP
    - Use netstat -s and look for error counters.

### 3.2常见瓶颈的例子

- IRQs没有得到正确的平衡。在某些情况下，irqbalance服务可能无法正常工作或根本无法运行。检查/proc/interrupts，并确保中断分布在多个CPU内核上。参考irqbalance手册，或手动平衡IRQs。在以下示例中，中断仅由一个处理器处理:
![](https://ws4.sinaimg.cn/large/006tNc79ly1fzoi25ke10j30gv0330t6.jpg)
- 看看除了/proc/net/softnet_stat的第一列之外，是否还有其他列在增加。在以下示例中，CPU0的计数器很大，需要增加预算:
![](https://ws1.sinaimg.cn/large/006tNc79ly1fzoi3nddx6j30h201xq2z.jpg)
- SoftIRQs可能没有足够的CPU时间来轮询适配器，如图1所示。使用sar、mpstat或top等工具来确定消耗CPU运行时间的因素。
- 使用ethtool -S ethX检查特定适配器的错误:
![](https://ws4.sinaimg.cn/large/006tNc79ly1fzoi4rcv94j30fz01nq2x.jpg)
- 数据正在补充到套接字缓冲区队列中，但没有足够快地排出。监控`ss -nmp`命令，寻找完整的RX队列。使用`netstat -s`命令，查找缓冲区修剪错误或UDP错误。以下示例显示UDP接收错误:
![](https://ws1.sinaimg.cn/large/006tNc79ly1fzoi7akngoj30gw02cq30.jpg)
- 通过不在应用程序中指定套接字缓冲区大小，增加应用程序的套接字接收缓冲区或使用缓冲区自动调整。检查应用程序是否调用setsockopt(SO_RCVBUF )，因为这将覆盖默认套接字缓冲区设置。
- 应用程序设计是一个重要因素。看看如何简化应用程序，使其更有效地从套接字读取数据。一种可能的解决方案是让单独的进程使用进程间通信( IPC )将套接字队列排到另一个进程，该进程可以像磁盘I/O一样完成后台工作
- 使用多个TCP流。传输数据时，流越多，效率越高。使用netstat - neopa检查应用程序正在使用的连接数量:-
 ![](https://ws2.sinaimg.cn/large/006tNc79ly1fzoia9soaxj30gr00xwel.jpg)
 - 使用较大的TCP或UDP数据包大小。每个单独的网络数据包都有一定的开销，例如报头。以较大的连续块发送数据将减少这种开销。这是通过使用send ( )和recv ( )函数调用指定更大的缓冲区大小来实现的；有关详细信息，请参见这些功能的手册页。

## 4. Performance Tuning
### 4.1 SoftIRQ Misses
如果SoftIRQs运行时间不够长，传入数据的速率可能会超过内核足够快地清空缓冲区的能力。结果，NIC缓冲区将溢出，流量将丢失。偶尔，有必要增加允许软寄存器在CPU上运行的时间。这被称为网络开发预算。预算的默认值为300。这将导致SoftIRQ进程在脱离CPU之前从NIC中释放300条消息:

![](https://ws1.sinaimg.cn/large/006tNc79ly1fzoidr7f9aj30i9012wei.jpg)

如果`/proc/net/softnet_stat`中的第三列增加，这个值可以翻倍，这表明SoftIRQ没有得到足够的CPU时间。小增量是正常的，不需要调整。

只有千兆接口的系统很少需要这种级别的调整。然而，超过10Gbps的系统可能需要增加这种可调性。
![](https://ws3.sinaimg.cn/large/006tNc79ly1fzoidsgpvdj30ia029dg0.jpg)

例如，将此NIC上的值从300调整到600将允许软中断运行两倍于默认CPU时间:
![](https://ws2.sinaimg.cn/large/006tNc79ly1fzoih3jdc4j30i300omx3.jpg)

### 4.2 Tuned
Tuned是一个自适应系统调优守护程序。它可用于将收集在一起的各种系统设置应用到称为配置文件的集合中。优化的配置文件可以包含指令，如CPU调控器、IO调度器，以及内核可调参数，如CPU调度或虚拟内存管理。Tuned还包括一个监控守护程序，可以控制或禁用CPU、磁盘和网络设备的节能能力。性能调整的目的是应用能够实现最理想性能的设置。tuned可以自动化这项工作的很大一部分。首先，安装tuned，启动tuning守护程序服务，并在启动时启用该服务:

```
sudo apt-get install tuned
service tuned start
tuned-adm list
```
![](https://ws4.sinaimg.cn/large/006tNc79ly1fzoimiyutrj30m30itad7.jpg)

可以在`/etc/tune-profiles/`目录中查看每个配置文件的内容。我们关心的是设置性能配置文件，如`throughput-performance`, `latency-performance`, or `enterprise-storage.`。

![](https://ws2.sinaimg.cn/large/006tNc79ly1fzoinet038j30i701h74c.jpg)
每次启动优化的服务时，都会应用选定的配置文件。详细的介绍请看 man tuned

### 4.3Numad
类似于tuned，numad是一个守护程序，它可以帮助具有非统一内存访问( NUMA )架构的系统进行进程和内存管理。Numad通过监控系统拓扑和资源使用来实现这一点，然后尝试定位进程以获得高效的NUMA局部性和效率，其中一个进程具有足够大的内存大小和CPU负载。
numad服务还要求启用cgroups ( Linux内核控制组)。

**需要时NUMA架构的设备才可用。**

### 4.4 IRQ Balance
IRQ平衡是一种服务，它可以根据实时系统条件自动平衡CPU内核之间的中断。正确版本的irqbalance为特定内核运行是至关重要的。更多信息请使用man irqbalance

### 4.5Manual balancing of interrupts
如果需要，IRQ也可以手动平衡。Red Hat强烈建议使用irqbalance来平衡中断，因为它根据系统使用情况和其他因素动态平衡中断。然而，手动平衡中断可用于确定irqbalance是否没有以最佳方式平衡IRQs，从而导致数据包丢失。在某些非常特殊的情况下，手动永久平衡中断可能是有益的。在这种情况下，中断将使用SMP关联性手动与CPU相关联。

有两种方法可以做到这一点:使用位掩码或使用`smp_affinity_list`，该列表可从Red Hat Enterprise Linux 6上获得。

想要手动平衡中断，需要关闭irqbalance服务。

查看允许接收设备中断的CPU内核:
![](https://ws4.sinaimg.cn/large/006tNc79ly1fzoj1uqt92j30i403xjs0.jpg)

手动平衡CPU内核的一种方法是使用脚本。概念验证示例:

![](https://ws1.sinaimg.cn/large/006tNc79ly1fzoj362uj8j30i705lmxt.jpg)

![](https://ws2.sinaimg.cn/large/006tNc79ly1fzoj37qcd4j30io038aal.jpg)

### 4.6 Ethernet Flow Control (a.k.a. Pause Frames)
暂停帧是适配器和交换机端口之间的以太网级流量控制。当RX或TX缓冲区已满时，适配器将发送“暂停帧”。开关将在毫秒或更短的时间内停止数据流动。这通常足够让内核清空接口缓冲区，从而防止缓冲区溢出和后续数据包丢失或溢出。理想情况下，交换机将在暂停时间内缓冲传入的数据。然而，重要的是要认识到这种流量控制水平仅在开关和适配器之间。如果数据包丢失，较高层(如TCP )或UDP或多播情况下应用程序应该启动恢复。

要使此功能生效，需要在NIC和交换机端口上启用暂停帧和流量控制。有关如何在端口上启用流量控制的说明，请参阅网络设备手册或供应商。
![](https://ws3.sinaimg.cn/large/006tNc79ly1fzoj5y2a1gj30j008jq3t.jpg)

### 4.7 Interrupt Coalescence (IC)
中断合并是指网络接口在发出硬中断之前将接收的流量，或者在接收流量之后经过的时间。过早或过频繁地中断会导致系统性能下降，因为内核会停止(或“中断”)正在运行的任务来处理来自硬件的中断请求。如果中断太晚，可能会导致流量没有足够快地从NIC中移除。可能会有更多的流量到达，覆盖仍在等待接收到内核的先前流量，导致流量损失。

大多数现代网卡和驱动程序都支持IC，许多允许驱动程序自动调节硬件产生的中断数量。IC设置通常包括两个主要组件，时间和数据包数量。时间是NIC在中断内核之前等待的微秒数( u-secs )，该数是在中断内核之前允许在接收缓冲区中等待的最大数据包数。

NIC的中断合并可以使用ethtool -C ethX命令查看，并使用ethtool - C ethX命令进行调整。自适应模式使卡能够自动调节IC。在自适应模式下，驱动将检查流量模式和内核接收模式，并实时估计合并设置，以防止数据包丢失。当接收到许多小数据包时，这很有用。较高的中断合并比延迟更有利于带宽。VOIP应用程序(延迟敏感)可能需要比文件传输协议(吞吐量敏感)更少的合并。不同品牌和型号的网络接口卡具有不同的功能和默认设置，因此请参阅制造商的适配器和驱动程序文档。

在此系统上，默认情况下启用自适应RX :
![](https://ws4.sinaimg.cn/large/006tNc79ly1fzoja1znjjj30ia04vmxn.jpg)
以下命令关闭自适应IC，并告诉适配器在接收到任何流量时立即中断内核:
![](https://ws3.sinaimg.cn/large/006tNc79ly1fzojaf3o4pj30i900rt8o.jpg)
一个现实的设置是，在中断内核之前，允许至少一些数据包缓冲在NIC中，并且至少有一段时间可以通过。有效范围可以从1到数百，这取决于系统能力和接收的流量。

### 4.8 The Adapter Queue
`netdev_max_backlog`是Linux内核中的一个队列，在从NIC接收流量之后，但是在协议栈( IP、TCP等)处理流量之前，流量被存储在该队列中。每个CPU内核有一个积压队列。给定核心的队列可以自动增长，包含的数据包数量最多可达`netdev_max_backlog`设置指定的最大值。`netif_receive_skb ( )`内核函数将为数据包找到相应的CPU，并将数据包排入该CPU的队列。如果该处理器的队列已满，并且已经达到最大大小，数据包将被丢弃。

要优化此设置，首先确定积压是否需要增加。

`/proc/net/softnet_stat`文件在第二列中包含一个计数器，当netdev backlog queue 溢出时，该计数器会递增。如果此值随时间递增，则需要增加`netdev_max_backlog`。

softnet_stat文件的每一行都代表一个从CPU0开始的CPU内核:
![](https://ws1.sinaimg.cn/large/006tNc79ly1fzojekr50bj30i801d748.jpg)

显示系统有几个cpu：
![](https://ws2.sinaimg.cn/large/006tNc79ly1fzojgjd9acj30ii016t8n.jpg)

当数据包无法放入积压队列时，将执行以下代码，其中get_cpu_var标识适当的处理器队列:
![](https://ws4.sinaimg.cn/large/006tNc79ly1fzojgsg3n6j30ia010q2v.jpg)

然后，上述代码递增该队列的丢弃统计信息。

softnet_stat文件中的每一行代表该CPU的netif_rx_stats结构。该数据结构包含:
![](https://ws3.sinaimg.cn/large/006tNc79ly1fzoji7yutbj30ie03eaaa.jpg)
第一列是中断处理程序接收的帧数。

第二列是由于超过netdev_max_backlog而丢弃的帧数。

第三列是ksoftirqd在仍有工作要做时超出netdev_budget或CPU时间的次数。

其他列可能会因版本而异。使用以下示例，CPU0和CPU1的以下计数器是前两行:
![](https://ws4.sinaimg.cn/large/006tNc79ly1fzojjf1lx6j30hy02djrj.jpg)
对于上面的例子，netdev_max_backlog不需要改变，因为丢弃的次数保持在0 

每列中的统计信息以十六进制形式提供。

默认的netdev_max_backlog值是1000。然而，对于以1Gbps操作的多个接口，甚至对于以10Gbps操作的单个接口来说，这可能还不够。尝试将此值加倍，并观察/proc/net/softnet_stat文件。如果加倍该值会降低下降增量的速率，请再次加倍并再次测试。重复此过程，直到确定最佳尺寸，并且drops不会增加。
![](https://ws4.sinaimg.cn/large/006tNc79ly1fzojlpg94gj30i9034dg3.jpg)
可以使用以下命令更改backlog，其中X是要设置的期望值:
![](https://ws1.sinaimg.cn/large/006tNc79ly1fzojqnftisj30i700kwef.jpg)

### 4.9Adapter RX and TX Buffer Tuning

适配器缓冲区默认值通常设置为小于最大值。通常，增加接收缓冲区大小就足以防止数据包丢失，因为这可以让内核有更多的时间来清空缓冲区。因此，这可以防止可能的数据包丢失。

以下接口有8千字节的缓冲空间，但仅使用1千字节:
![](https://ws1.sinaimg.cn/large/006tNc79ly1fzojyfj42ej30ib04xjrs.jpg)

最大限度地增加RX和TX缓冲区:
![](https://ws1.sinaimg.cn/large/006tNc79ly1fzojzber3wj30ie00ojrb.jpg)


**适配器传输队列长度**

传输队列长度值决定了传输前可以排队的数据包数量。默认值1000通常适用于当今高速10Gbps甚至40Gbps网络。但是，如果适配器上的传输错误数量正在增加，请考虑加倍。使用ip -s link查看适配器的TX队列中是否有任何丢弃。
![](https://ws1.sinaimg.cn/large/006tNc79ly1fzok3tm5xrj30i403bgm8.jpg)

队列长度可以通过ip链接命令修改
![](https://ws4.sinaimg.cn/large/006tNc79ly1fzok40mxi1j30ib028jrq.jpg)

要在重新启动后保存此值，可以编写udev规则，以便在创建接口时将队列长度应用于接口，或者可以使用`/sbin/ifup-local`的脚本扩展网络脚本，如知识库中所述:

**Module parameters**

每个网络接口驱动程序通常都是可加载内核模块。可以使用modprobe命令加载和卸载模块。这些模块通常包含可用于进一步调整设备驱动程序和NIC的参数。`modinfo <drivername >`命令可用于查看这些参数。

Linux内核通过sysfs路径`/ sys / module / < drivername > /parameters`导出模块参数的当前设置。

具体情况细节看pdf文档。

**Adapter Offloading**
为了减少系统的CPU负载，现代网络适配器具有卸载功能，将一些网络处理负载转移到网络接口卡上。例如，内核可以向网卡提交大的(高达64k) TCP数据段，然后网卡会将这些数据段分解为MTU大小的数据段。这一特殊功能称为TCP分段卸载( TSO )。

默认情况下，卸载功能通常是启用的。深入介绍每个卸载功能超出了本文档的范围，但是，当系统网络性能不佳，需要重新测试时，关闭这些功能是一个很好的故障排除步骤。如果性能有所提高，理想情况下，将更改范围缩小到特定卸载参数。

卸载设置由ethtool - K ethX管理。常见设置包括:
- GRO: Generic Receive Offload 
- LRO: Large Receive Offload 
- TSO: TCP Segmentation Offload 
- RX check-summing = Processing of receive data integrity 
- TX check-summing = Processing of transmit data integrity (required for TSO)

###4.10 Jumbo Frames

默认的802.3以太网帧大小为1518字节，或带有VLAN标记的1522字节。以太网报头消耗了其中的18个字节(或者VLAN标记为22个字节)，留下了1500字节的有效最大有效负载。巨型帧是以太网的非官方扩展，它网络设备供应商已经制定了一个事实上的标准，将有效载荷从1500字节增加到9000字节。

对于常规以太网帧，放在线路上的每1500字节数据有18字节的开销，即1.2 %的开销。

对于巨型帧，放在线路上的每9000字节数据有18字节的开销，或者0.2 %的开销。

上述计算假设没有VLAN标签，但是这样的标签会增加4个字节的开销，从而提高效率。

当传输大量连续数据时，例如在两个系统之间发送大文件，使用巨型帧可以获得上述效率。当传输少量数据时，例如通常低于1500字节的web请求，使用较大的帧大小可能没有好处，因为通过网络传输的数据将包含在小帧中。

对于要配置的巨型帧，网段(即广播域)中的所有接口和网络设备必须支持巨型帧，并启用增加的帧大小。有关增加帧大小的说明，请咨询网络交换机供应商。

在接口的/etc/sysconfig/network-scripts/ifcfg文件中，用MTU=9000增加帧大小。
可以使用ip链接命令检查MTU :
![](https://ws4.sinaimg.cn/large/006tNc79ly1fzokezny5xj30i4028mxk.jpg)

### 4.11 UDP Buffer Tuning
UDP是一种远不如TCP复杂的协议。由于UDP不包含会话可靠性，因此应用程序有责任识别并重新传输丢弃的数据包。没有窗口大小的概念，丢失的数据不会被协议恢复。唯一可用的调整包括增加接收缓冲区大小。但是，如果`netstat -us`报告错误，另一个潜在问题可能是阻止应用程序清空其接收队列。如果netstat -us显示“数据包接收错误”，请尝试增加接收缓冲区并重新测试。由于其他原因，该统计数据也可以增加，例如有效负载数据小于UDP报头建议的短分组，或者校验和计算失败的损坏分组，因此如果缓冲区调整不能解决UDP接收错误，可能需要更深入的调查。

UDP缓冲区可以类似于最大TCP缓冲区的方式进行调整:
![](https://ws4.sinaimg.cn/large/006tNc79ly1fzokssmksej30i701kq2z.jpg)
更改最大大小后，新设置需要重新启动应用程序才能生效。

### 4.12 Identifying Interrupts to Balance

检查适配器上的RX和TX队列数量:
![](https://ws3.sinaimg.cn/large/006tNc79ly1fzokvblf6jj30i902z0t7.jpg)
加载NIC驱动程序模块时会分配队列。在某些情况下，可以使用ethtool - L命令在线动态分配队列数量。上述设备有4个接收队列和1个发送队列。

每个网络驱动程序的统计数据都不一样，但是如果网络驱动程序提供单独的队列统计数据，这些可以通过命令ethtool - S ethX看到，其中ethX是有问题的接口:
![](https://ws1.sinaimg.cn/large/006tNc79ly1fzokwbkbyvj30i103o3ys.jpg)

**RSS :接收侧缩放**
RSS受到许多常见网络接口的支持。在接收数据时，NIC可以将数据发送到多个队列。每个队列可以由不同的CPU提供服务，允许高效的数据检索。RSS充当驱动程序和卡固件之间的API，以确定数据包是如何分布在CPU核心上的，其思想是将流量导向不同CPU的多个队列可以实现更快的吞吐量和更低的延迟。RSS控制哪个接收队列获得任何给定的数据包，无论卡是否侦听特定单播以太网地址，它侦听哪个多播地址，哪个队列对或以太网队列获得多播数据包的副本。

## 参考链接

- [Network performance monitoring and tuning in Linux](https://opensourceforu.com/2016/10/network-performance-monitoring/)
- [How do I run a script or program immediately after my network interface goes up](https://access.redhat.com/solutions/8694)
