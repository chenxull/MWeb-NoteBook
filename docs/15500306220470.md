#Kubernetes初认识
## Kubernetes是个什么样的项目

Kubernetes的很多概念和架构是基于google过去十多年设计构建和管理大规模容器集群的经验。

K8s还是一个管理跨主机容器化应用的系统，实现了包括应用部署，高可用管理和弹性伸缩在内的一系列基础功能并封装成一套完整，简单易用的RESTful API对外提供服务。 其设计哲学之一就是维护应用容器集群一直处于用户所期望的状态，为了实现这一理念，它建立了一套健壮的集群自恢复机制，包括容器的重启，自动重调度以及自动备份。

## Pod设计解读

在Kubernetes中，能够被创建，调度和管理的最小单元是pod，而非单个容器。一个pod可以由多个docker容器构成，pod意思是豆荚，里面容纳了许多豆子，很形象。pod里面的容器共享network namspace，并通过volum机制共享一部分存储。同属于一个pod的容器还共享一下的namespace：IPC 容器能够使用system V IPC或POSIX消息队列进行通信 ,UTS 同一个pod内的容器应用共享主机名。

当我们把负责不同职责的进程合并在同一个容器中时，就是去了微服务的优势，比如说我们已经无法做到位每个模块单独开发，部署和运维，任何一个进程的代码如果需要更新，都得更新整个容器镜像，无法实现增量部署，滚动更新。此外，现在这些进程都在一起运行，一旦哪个进程有内存泄露之类的问题，整个容器都有被拖垮的风险。

## label和label selector与pod协作

labels属性是一组绑定到kubernetes对象pod上的键值对，同一个对象的label属性的key必须独一无二。

利用一个label的key代表一个资源管理维度，不同的kubernetes对象携带一个或一组相同的label，是其实现多维度资源管理的精华所在。

label selector是kubernetes核心的分组机制，通过label selector，客户端或用户能够识别一组有共同特征或属性的kubernetes对象。label selector经常作为发送给APIServer的RESTful查询请求的条件参数，用于检索一个与label selector匹配的kubernetes对象列表。

## pod的现状和未来走向

## 资源共享和通信

目前pod内的容器共享同一个network namespace，IP资源和端口区间，未来pod内的容器之间将能够共享CPU和内存，这意味着将来pod内的容器可以完美的实现一种亲密的关系，就像在虚拟机里部署和启动多个应用的过程一样。部署起来更为灵活。

## 集中式管理

目前而言，pod对于kubernetes来说最具有价值就是原子化调度，即在为一个pod选择目的的宿主机时，kubernetes会考量这个机器是否能够放下整个pod，而避免 出现本应该部署在一个的容器因为资源不足无法满足放置在同一个pod中。

## pod的使用场景

- 一个内容管系统，包括文件和数据加载器，本地缓存管理系统的组合
- 一个常规应用和它的日志和检查点备份压缩，轮换，快照系统等组合
- 一个常规应用和它的数据变化检测器，日志实时收集，事件发布器等组合。

总而言之，尽量不要在单个pod中运行同一个应用的多个实例，因为pod设计的目的就是用于不用应用之间的协同，所以把一个应用的多个副本部署在同一个pod中很不合适。

## replication controller设计解读

kubernetes中第二个重要的概念就是replication controller，它决定了一个pod有多少同时运行的副本，并保证这些副本的期望状态与当前状态一致。replication controller就相当于一个pod的守护者一样，维护其状态。

replication controller在设计上依然体现出了旁路控制的思想，在kubernetes中没有像cloud Foundry那样设置一个专门的健康检查组件，而是为每个pod外挂一个控制器进程。这样做的一个好处就是避免健康检查组件成为性能的瓶颈；即使这个控制器进程失效，容器依然可以正常运行，pod和容器无需知道这个控制器，也不会把这个控制器当做依赖。

pod的状态是replication controller进行上述控制的唯一依据。

## pod的状态转换

根据设定的执行策略，对于不同状态的pod，采用不同的应对方式，来符合要求。

replication controller具体来说有一下功能:

- 维护它所控制的pod的数量，如果需要调整pod的数量，则通过修改它的副本数量字段来实现。
- 在pod模板中定义replication controller所控制的pod的labels，使用replica selector匹配pod集，实现对pod的管理。

replication controller不负责调度pod，检查pod是否与指定的pod模板匹配等工作，应为这样会阻塞replication controller的弹性压缩和其他自动化操作的进程。

## service的设计解读

由于重新调度的原因，pod在kubernetes中的IP地址不是固定的，因此需要一个代理来确保需要使用pod的应用不需要知道pod的真实IP地址，还有一个原因就是当使用replication controller创建多个pod副本时，需要一个代理为这些pod做负载均衡。

service主要由一个IP地址和一个label selector组成，在创建之初，每个service便被分配一个独一无二的IP地址，该IP地址与service的生命周期相同且不再更改。

## service的工作原理

kubernetes集群中的每个节点上都运行着一个服务代理（kube-proxy），它是负责实现service的主要组件。

kube-proxy有userspace和iptables二种工作模式

## userspace模式

对于每个service，kube-proxy都会在宿主机上随机监听一个端口与这个service对应起来，并且在宿主机上建立起iptables规则，将service IP ：service port的流量重定向到上述端口。

这样所有发往service IP：service port的流量都会经过iptables重定向到它对应的随机端口上，在经过kube-proxy的代理到某个后端pod。kube-proxy里会维护本地端口与service的映射关系，以及service代理的pod清单，至于选择哪个pod，由路由策略等决定。

## iptables模式

kube-proxy将只负责创建和维护iptables的路由规则，其余的工作均由内核态的iptables完成，与userspace的kube-proxy相比，速度更快，可靠性更高。

## 新一代副本控制器 replica set（没细看）

replica set引入了对基于子集的selector查询。

## Deployment（没细看）

Deployment多用于为pod和replica set提供更新，并且可以方便的跟踪其所属的replica set或者pod数量及状态的变化，Deployment是为了应用的更新设计的。

## DaemonSet

在生产环境中，我们希望每个工作节点上都运行着某个相同的pod副本，如下：

- 每个工作节点上都运行一个存储daemon
- 每个工作节点上都运行一个日志收集daemon
- 每个工作节点上都运行一个监控daemon

要实现上述功能一种方法就是在每个工作节点注册到集群时手动将pod绑定到节点上，或者采用其他daemon直接进行管理。daemonset的作用在于，每当有一个新的工作节点加入集群中，系统就会按照daemonset的配置在节点上运行相应的pod。

## ConfigMap

很多生产环境中的应用程序配置较为复杂，可能需要多个config文件，命令行参数和环境变量的组合。这些配置信息应该从应用程序镜像中解耦出来，以保证镜像的可移植性以及配置信息不被泄露。ConfigMap正是用来完成这一工作的。

ConfigMap包含一系列的键值对，用于存储被pod或系统组件访问的信息

## Job

在kubernetes诞生之初，预设的服务场景基本上围绕long-running service，如web service。 在后来的发展中，kubernetes有了专门支持bath job的资源-Job，我们可以将其理解为run to completion的任务，它对应的restart policy 为OnFailure或Never的应用。

## Horizontal Pod Autoscaler

自动扩展分为二种，其一为水平扩展，正对于实例数目的增减。其二为垂直扩展，即单个实例可以使用资源的增减。

# Kubernetes核心组件解读

kubernetes由二种节点组成，master节点和工作节点。前者是管理节点后者是容器运行的节点。

master节点主要有3个重要的组件分别为APIServer，scheduler和controller manager。 APIServer组件负责响应用户的管理请求，进行指挥协调等工作；scheduler的作用是将待调度的pod绑定到合适的工作节点上；controller manager是一组控制器的集合，负责控制管理对应的资源，如副本和工作节点等。

工作节点上运行二个重要的组件，分别为kubelet何kube-proxy，前者可以看成一个管理维护pod运行的agent，后者负责将service的流量转发到对应的endpoint。

kubernetes的架构体现了很多分布式系统设计的最佳实践，如组件之间的松耦合，各个组件之间不直接存在依赖关系，都是通过APIServer进行交互。

![](https://ws1.sinaimg.cn/large/006tNbRwly1fwtqe2syi3j31260ks42b.jpg)

## APIServer

kubernetes APIServer负责对外提供 kubernetes API服务，它运行在kubernetes的master节点中。作为系统管理指令的统一入口，APIServer担负着统揽全局的重任，任何对资源进行增删改查的操作都要交给APIServer处理后才能提交给etcd。

APIServer总体上由二部分组成:HTTP/HTTPS服务和一些功能性插件。

## APIServer的职能

作为集群的全局掌控者，主要负责一下5方面的工作。

- 对外提供基于RESTful的管理接口，支持对pod，service，replication controller，工作节点的增删改查。
- 配置kubernetes的资源对象，并将这些资源对象的期望转态和当前实际存储在etcd中供kubernetes其他组件读取和分析。
- 提供可定制的功能性插件，完善对集群的管理。
- 系统日志收集功能，暴露在logs api
- 可视化的API

## APIServer对etcd的封装

kubernetes使用etcd作为后台存储解决方案，而APIServer则基于etcd实现了一套RESTful API，用于操作存储在etcd中的Kubernetes对象实例。

## scheduler

kubernetes scheduler是一个典型的单体调度器。他的作用是根据特定的调度算法将pod调度到指定的工作节点上，这一过程通常被称为绑定（bind）。

scheduler的输入是待调度pod和可用的工作节点，输出是应用调度算法从列表中选择一个最优的用于绑定待调度pod的节点。

![](https://ws4.sinaimg.cn/large/006tNbRwly1fwtriopat0j30pi06wwf3.jpg)

## scheduler数据采集模型

kubernetes中没有消息系统来帮助用户实现各组件间的高效通信，这使得scheduler需要定时的向APIServer获取各种它感兴趣的数据，比如已调度，待调度的pod信息，node状态列表，service对象信息，这会给apiserver带来很大的访问压力。

scheduler为那些感兴趣的资源和数据设置了本地缓存机制，以避免一刻不停的轮询apiserver带来的额外性能开销。机制分为二种：简单的cache对象；先进先出的队列。

输入到scheduler中的数据

![](https://ws1.sinaimg.cn/large/006tNbRwly1fwtrqv8njfj316g0een0w.jpg)

## scheduler调度算法

kubernetes的调度算法都是以如下的方式来描述的：

func RegisterAlgorithmProvider(name string,predicateKeys,priorityKeys sets.string) string

其中第一个参数为算法名，第2个和第3个组成算法的调度策略。 kubernetes的调度策略分为二个阶段，predicates和priorities，predicates回答能不能，priorities在回答能的基础上，为候选节点设置优先级，描述适合的程度有多高。

## controller manager（待看）

运行在集群的master节点上，是基于pod API上的一个独立的服务，它管理着kubernetes集群中各种控制器。

## 服务端点控制器

## 副本管理控制器

## 垃圾回收控制器

## 节点控制器

## 资源分配控制器

## Kubelet

Kubelet是kubernetes集群工作节点上最重要的组件进程，它负责管理和维护在这台主机上运行的所有容器。它担任一个工作节点上所有容器司令官的角色，虽然整个过程看起来很复杂，但是本质上kubelet对工作节点上的二种更新做出相应的行为反馈，其一为pod spec的更新，其二为容器实际运行状态的更新。所以kubelet是为了获取或同步二种更新所设计的。

## kubelet与cAdvisor的交互

cAdvisor作为抓取docker容器和宿主机资源信息的工具，主要负责收集工作节点上的容器信息和宿主机信息。

## kubelet垃圾回收机制

主要涵盖二个方面：容器回收和镜像回收。

## kube-proxy

# Kubernetes存储核心原理