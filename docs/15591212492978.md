# HPC Container 的 dockerfile 生成器 [英伟达](https://devblogs.nvidia.com/making-containers-easier-with-hpc-container-maker/)

从头开始构建高性能计算容器将面临一系列挑战。通常由高性能计算数据中心提供的部分软件环境必须重新部署到容器中。对于那些习惯于只加载相关环境模块的人来说，从头开始安装编译器、MPI库、CUDA和其他核心高性能计算组件可能是令人生畏的。

![](https://devblogs.nvidia.com/wp-content/uploads/2018/05/Making-Containers-Easier-with-HPC-Container-Maker-Hero-Image.png)

## 介绍 dockerfile 生成器
`HPC Container Maker (HPCCM) `是一个开源项目，它解决了创建高性能计算应用程序容器的挑战。`HPCCM`将部署核心高性能计算组件的最佳实践与容器最佳实践封装到模块化构建模块中，以减少容器开发工作量，最小化映像大小，并利用映像分层。

HPCCM在GitHub上可用，它通过将容器镜像中应该包含的内容与如何配置、构建和安装组件的规范细节分开，使得创建高性能计算应用程序容器变得更加容易。这种分离还使得高性能计算组件部署的最佳实践能够随着时间透明地发展。

HPCCM附带了用于`GNU`和`PGI`社区版编译器、`OpenMPI`和`MVAPICH2 MPI`库、用于`InfiniBand`支持的`Mellanox OpenFabrics`企业分发`(OFED)`、`FFTW`和`HDF5`库、`Python`等的构建模块。

## 如何工作
容器框架依赖于输入规范文件来定义相应容器图像的内容。输入规范文件是Docker和许多其他容器运行时的Dockerfile文件和`Singularity`的`Singularity`配方文件。规范文件包含构建容器映像时要执行的一组精确步骤。对于给定的组件，这些步骤可以包括下载源代码、配置和构建、安装以及最终清理。使用HPCCM构建块，很容易为Docker和`Singularity`生成优化的容器规范文件。

例如，以下Dockerfile和`Singularity`配方文件是使用包含的`OpenMPI`构建块从相同的两行高级HPCCM配方生成的。OpenMPI构建块有许多配置选项，下面只显示了其中的一些选项，用于定制配置、构建和安装。

`Basic HPCCM recipe`
```python
Stage0 += baseimage(image='nvidia/cuda:9.0-devel', _as='devel')
Stage0 += openmpi(cuda=True, infiniband=True, prefix='/usr/local/openmpi',
                  version='3.0.0')
```

`Dockerfile generated from the basic HPCCM recipe`

```dockerfile
FROM nvidia/cuda:9.0-devel AS devel

# OpenMPI version 3.0.0
RUN apt-get update -y && \
    apt-get install -y --no-install-recommends \
        file \
        hwloc \
        openssh-client \
        wget && \
    rm -rf /var/lib/apt/lists/*
RUN mkdir -p /tmp && wget -q --no-check-certificate -P /tmp https://www.open-mpi.org/software/ompi/v3.0/downloads/openmpi-3.0.0.tar.bz2 && \
    tar -x -f /tmp/openmpi-3.0.0.tar.bz2 -C /tmp -j && \
    cd /tmp/openmpi-3.0.0 && ./configure --prefix=/usr/local/openmpi --disable-getpwuid --enable-orterun-prefix-by-default --with-cuda --with-verbs && \
    make -j4 && \
    make -j4 install && \
    rm -rf /tmp/openmpi-3.0.0.tar.bz2 /tmp/openmpi-3.0.0
ENV PATH=/usr/local/openmpi/bin:$PATH \
    LD_LIBRARY_PATH=/usr/local/openmpi/lib:$LD_LIBRARY_PATH
```

`Singularity recipe file generated from the basic HPCCM recipe`

```
BootStrap: docker
From: nvidia/cuda:9.0-devel

# OpenMPI version 3.0.0
%post
    apt-get update -y
    apt-get install -y --no-install-recommends \
        file \
        hwloc \
        openssh-client \
        wget
    rm -rf /var/lib/apt/lists/*
%post
    mkdir -p /tmp && wget -q --no-check-certificate -P /tmp https://www.open-mpi.org/software/ompi/v3.0/downloads/openmpi-3.0.0.tar.bz2
    tar -x -f /tmp/openmpi-3.0.0.tar.bz2 -C /tmp -j
    cd /tmp/openmpi-3.0.0 && ./configure --prefix=/usr/local/openmpi --disable-getpwuid --enable-orterun-prefix-by-default --with-cuda --with-verbs
    make -j4
    make -j4 install
    rm -rf /tmp/openmpi-3.0.0.tar.bz2 /tmp/openmpi-3.0.0
%environment
    export PATH=/usr/local/openmpi/bin:$PATH
    export LD_LIBRARY_PATH=/usr/local/openmpi/lib:$LD_LIBRARY_PATH
```

### `Base Recipes`作为 HPC 应用软件容器的起点
HPCCM容器包括几个“基本容器”配方，是 HPC 容器的一个极好的起点。这些基本配方包括基本的高性能计算组件，如编译器、MPI库、CUDA、Mellanox OFED，以及在高性能计算软件堆栈中常见的其他组件。

劳伦斯利弗莫尔国家实验室(LLNL)的“MPI带宽”样本程序将作为一个代理应用程序，说明如何使用基于高性能计算机控制模块的配方来创建一个应用程序容器。

首先，从其中一个基础镜像生成一个Dockerfile，并使用Docker创建一个名为`baseimage`的本地容器镜像。构建可能需要一段时间，因为需要下载和构建许多高性能计算组件。请注意，这些步骤假设用户是`docker`组的一部分，因此不需要在每个docker命令前面加上sudo

```
 hpccm.py --recipe recipes/hpcbase-gnu-openmpi.py --single-stage > Dockerfile.baseimage
$ docker build -t baseimage -f Dockerfile.baseimage .
```

使用新创建的基本容器镜像作为应用程序`application`的起点。在这种情况下，当起始点是一个带有MPI库的容器镜像时，构建MPI Bandwidth 代理应用程序的文档文件是微不足道的；只需将源文件复制到容器中，并使用MPI编译器包装器构建它。使用以下内容创建一个名为`Dockerfile.baseimage-mpi_bandwidth`的文件。

使用Docker构建应用程序容器映像。


```
docker build -t baseimage-mpi_bandwidth -f Dockerfile.baseimage-mpi_bandwidth .
```


最后，运行集装箱化的MPI带宽代理应用程序。

```
$ nvidia-docker run --rm -ti baseimage-mpi_bandwidth mpirun --allow-run-as-root -n 2 /workspace/mpi_bandwidth

******************** MPI Bandwidth Test ********************
Message start size= 100000 bytes
Message finish size= 1000000 bytes
Incremented by 100000 bytes per iteration
Roundtrips per iteration= 100
MPI_Wtick resolution = 1.000000e-09
************************************************************
task    0 is on dbcb15aff446 partner=   1
task    1 is on dbcb15aff446 partner=   0
************************************************************
...
***Message size:  1000000 *** best /  avg / worst (MB/sec)
   task pair:    0 - 1: 8754.23 / 8641.50 / 6640.06
   OVERALL AVERAGES:          8754.23 / 8641.50 / 6640.06
```

或者，构建MPI Bandwidth proxy 应用程序的步骤可以添加到单个独立的高性能计算应用程序`Dockerfile`的`Dockerfile.baseimage`的末尾。这样做还将允许从通用基础映像中移除未使用的组件，以减小应用程序容器映像的大小。 

## Portable HPC Application Recipes
HPCCM还包括一些流行应用的参考HPC应用配方，如`GROMACS`和`MILC`。这些可以按原样用于生成相应的高性能计算应用程序容器映像。请注意，与通用基本容器配方相比，这些配方是为每个应用程序专门定制的，只包含必要的组件。

MPI带宽代理应用程序将再次用于演示如何创建便携式高性能计算应用程序配方。

```
$ cp recipes/hpcbase-gnu-openmpi.py mpi_bandwidth.py
```
将以下内容添加到mpi_bandwidth.py的末尾。这将执行与上一节相同的步骤，将源文件复制到容器中，并使用mpi编译器包装器构建它。

```
# MPI bandwidth "application"
Stage0 += copy(src='mpi_bandwidth.c', dest='/tmp/mpi_bandwidth.c')
Stage0 += shell(commands=['mkdir -p /workspace',
                          'mpicc -o /workspace/mpi_bandwidth /tmp/mpi_bandwidth.c'])
Stage1 += copy(src='/workspace/mpi_bandwidth', dest='/workspace/mpi_bandwidth',
               _from=0)
```

现在为MPI带宽生成一个文档文件，并构建相应的容器映像。在Dockerfile.recipe的末尾，请注意生成的构建MPI带宽的Docker语法与前一节中`Dockerfile.baseimage-mpi_bandwidth`中的内容相同。


到目前为止，已经在容器内部调用了MPI，并使用共享内存在MPI等级之间进行通信。容器也可以用于使用高性能互连(如InfiniBand)的多节点运行。多节点运行成功使用容器取决于其他因素，如主机网络和防火墙配置、容器权限以及使网络设备可由容器访问。有关更多信息，请参考您的容器框架文档。

## 总结
无论底层数据中心环境如何，容器都可以使高性能计算应用程序得到广泛部署。英伟达通过在英伟达图形处理器云(NGC)上提供对已调优和测试过的高性能计算应用程序容器的访问，支持高性能计算容器的使用。在NGC没有容器的情况下，高性能计算控制模块简化了创建新的高性能计算应用程序容器。 高性能容器制造商(HPCCM)是一个开源项目，可在GitHub上获得。