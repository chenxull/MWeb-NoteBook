# PV ,PVC ,StorageClass 
其中，PV 描述的，是持久化存储数据卷。这个 API 对象主要定义的是一个持久化存储在宿主机上的目录，比如一个 NFS 的挂载目录。

而PVC 描述的，则是 Pod 所希望使用的持久化存储的属性。比如，Volume 存储的大小、可读写权限等等。


而用户创建的 PVC 要真正被容器使用起来，就必须先和某个符合条件的 PV 进行绑定。这里要检查的条件，包括两部分：

- 第一个条件，当然是 PV 和 PVC 的 spec 字段。比如，PV 的存储（storage）大小，就必须满足 PVC 的要求。
- 而第二个条件，则是 PV 和 PVC 的 `storageClassName` 字段必须一样。这个机制我会在本篇文章的最后一部分专门介绍。


在成功地将 PVC 和 PV 进行绑定之后，Pod 就能够像使用 hostPath 等常规类型的 Volume 一样，在自己的 YAML 文件里声明使用这个 PVC 了，如下所示：

```
apiVersion: v1
kind: Pod
metadata:
  labels:
    role: web-frontend
spec:
  containers:
  - name: web
    image: nginx
    ports:
      - name: web
        containerPort: 80
    volumeMounts:
        - name: nfs
          mountPath: "/usr/share/nginx/html"
  volumes:
  - name: nfs
    persistentVolumeClaim:
      claimName: nfs

```

PVC 和 PV 的设计，其实和面向对象的思想完全一致。

PVC 可以理解为持久化存储的接口，它提供了对某种持久化存储的描述，但不提供具体的实现；而这个持久化存储的实现部分则是由 PV 负责完成的。

**好处：**作为应用开发者，只需要和 pVC 这个接口打交道，不必关系存储背后实现的具体细节。

### 存在的问题  PersistentVolumeController
比如，你在创建 Pod 的时候，系统里并没有合适的 PV 跟它定义的 PVC 绑定，也就是说此时容器想要使用的 Volume 不存在。这时候，Pod 的启动就会报错。

但是，过了一会儿，运维人员也发现了这个情况，所以他赶紧创建了一个对应的 PV。这时候，我们当然希望 Kubernetes 能够再次完成 PVC 和 PV 的绑定操作，从而启动 Pod。

在 kubernetes中有专门处理**持久化存储的控制器，叫做 volume controller**，这个 Volume Controller 维护着多个控制循环，其中有一个循环，扮演的就是撮合 PV 和 PVC 的“红娘”的角色。它的名字叫作 `PersistentVolumeController`。

PersistentVolumeController 会不断地查看当前每一个 PVC，是不是已经处于 Bound（已绑定）状态。如果不是，那它就会遍历所有的、可用的 PV，并尝试将其与这个“单身”的 PVC 进行绑定。

### PV 对象如何实现 持久化存储
所谓容器的 Volume，其实就是将一个宿主机上的目录，跟一个容器里的目录绑定挂载在了一起。

**而所谓的“持久化 Volume”，指的就是这个宿主机上的目录，具备“持久性”**。即：这个目录里面的内容，既不会因为容器的删除而被清理掉，也**不会**跟当前的宿主机绑定。这样，当容器被重启或者在其他节点上重建出来之后，它仍然能够通过挂载这个 Volume，访问到这些内容。


之前使用的` hostpath`和` emptyDir`类型的 Volume 并不具备这个特性：它们既有可能被 kubelet 清理掉，也不能被迁移到其他节点上。


所以，大多数情况下，持久化 Volume 的实现，往往依赖于一个**远程存储服务**，比如：远程文件存储（比如，NFS、GlusterFS）、远程块存储（比如，公有云提供的远程磁盘）等等。


所谓**持久化，指的就是容器在这个目录里写入的文件，都会被保存到远程的存储中，从而使这个目录具备了持久性**


#### 准备持久化的步骤

当一个 Pod 调度到一个节点上之后，kubelet 就要负责为这个 Pod 创建它的 Volume 目录。默认情况下，kubelet 为 Volume 创建的目录是如下所示的一个宿主机上的路径：

```
/var/lib/kubelet/pods/<Pod 的 ID>/volumes/kubernetes.io~<Volume 类型 >/<Volume 名字 >

```

接下来，kubelet 要做的操作就取决于你的 Volume 类型了。

如果你的 Volume 类型是远程块存储，比如 Google Cloud 的 Persistent Disk（GCE 提供的远程磁盘服务），那么 kubelet 就需要先调用 Goolge Cloud 的 API，将它所提供的 Persistent Disk 挂载到 Pod 所在的宿主机上。

```
$ gcloud compute instances attach-disk < 虚拟机名字 > --disk < 远程磁盘名字 >

```

这一步**为虚拟机挂载远程磁盘的操作，对应的正是`二阶段处理`的第一阶段。在kubernetes 中，我们把这个阶段称为 Attach**

Attach 阶段完成后，为了能够使用这个远程磁盘，kubelet 还要进行第二个操作，即：**格式化这个磁盘设备，然后将它挂载到宿主机指定的挂载点上**。不难理解，这个挂载点，正是我在前面反复提到的 Volume 的宿主机目录。所以，这一步相当于执行：

```
# 通过 lsblk 命令获取磁盘设备 ID
$ sudo lsblk
# 格式化成 ext4 格式
$ sudo mkfs.ext4 -m 0 -F -E lazy_itable_init=0,lazy_journal_init=0,discard /dev/< 磁盘设备 ID>
# 挂载到挂载点
$ sudo mkdir -p /var/lib/kubelet/pods/<Pod 的 ID>/volumes/kubernetes.io~<Volume 类型 >/<Volume 名字 >

```

**这个将磁盘设备格式化并挂载到 Volume 宿主机目录的操作，对应的正是 二阶段处理 的第二个阶段，我们称为 Mount**

Mount 阶段完成后，这个 Volume 的宿主机目录就是一个“持久化”的目录了，容器在它里面写入的内容，会保存在 Google Cloud 的远程磁盘中。

**Kubernetes 又是如何定义和区分这两个阶段的呢？**

在具体的` Volume`插件的实现接口上，kubernetes 分别给这二个阶段提供了不同的参数列表：
- 对于第一阶段 Attach，kubernetes 提供的可用参数是 nodeName，即宿主机的名字
- 第二阶段 Mount，kubernetes 提供的可用参数是 dir，即 Volume 的宿主机目录。

而经过了“两阶段处理”，我们就得到了一个“持久化”的 Volume 宿主机目录。所以，接下来，kubelet 只要把这个 Volume 目录通过 CRI 里的 Mounts 参数，传递给 Docker，然后就可以为 Pod 里的容器挂载这个“持久化”的 Volume 了。其实，这一步相当于执行了如下所示的命令：

```
$ docker run -v /var/lib/kubelet/pods/<Pod 的 ID>/volumes/kubernetes.io~<Volume 类型 >/<Volume 名字 >:/< 容器内的目标目录 > 我的镜像 ...

```

**小结**，整体上分为二个阶段：
- 远程目录挂载到宿主机
    - Attach：虚拟机挂载远程磁盘
    - Mount：宿主机上的指定挂载点 挂载到远程磁盘
- 通过 CRI，将目录挂载到容器中


所以，在 Kubernetes 中，**上述关于 PV 的“两阶段处理”流程，是靠独立于 kubelet 主控制循环（Kubelet Sync Loop）之外的两个控制循环来实现的。**

“第一阶段”的 Attach（以及 Dettach）操作，是由 `Volume Controller` 负责维护的，这个控制循环的名字叫作：`AttachDetachController`。而它的作用，就是不断地检查每一个 Pod 对应的 PV，和这个 Pod 所在宿主机之间挂载情况。从而决定，是否需要对这个 PV 进行 Attach（或者 Dettach）操作。

而“第二阶段”的 Mount（以及 Unmount）操作，**必须发生在 Pod 对应的宿主机上，所以它必须是 kubelet 组件的一部分**。这个控制循环的名字，叫作：`VolumeManagerReconciler`，它运行起来之后，是一个独立于 kubelet 主循环的 Goroutine。


通过这样将 Volume 的处理同 kubelet 的**主循环解耦**，Kubernetes 就避免了这些耗时的远程挂载操作拖慢 kubelet 的主控制循环，进而导致 Pod 的创建效率大幅下降的问题。**实际上，kubelet 的一个主要设计原则，就是它的主控制循环绝对不可以被 block。**这个思想，我在后续的讲述容器运行时的时候还会提到。

### StorageClass

PV对象的创建时由运维人员完成的，但在实际环境中这是非常复杂的。

这是因为，一个大规模的 Kubernetes 集群里很可能有成千上万个 PVC，这就意味着运维人员必须得事先创建出成千上万个 PV。更麻烦的是，随着新的 PVC 不断被提交，运维人员就不得不继续添加新的、能满足条件的 PV，否则新的 Pod 就会因为 PVC 绑定不到 PV 而失败。在实际操作中，这几乎没办法靠人工做到。

所以，Kubernetes 为我们提供了一套可以**自动创建 PV 的机制**，即：`Dynamic Provisioning`。

相比之下，之前人工管理 PV 的方式叫做 `Static Provisioning`

Dynamic Provisioning 机制工作的核心，在于一个名叫 `StorageClass` 的 API 对象。

**StorageClass 对象的作用，其实就是创建 pv 的模板**。具体来说，storageClass 对象会定义如下二个部分内容：
- PV 属性，比如存储类型，Volume 的大小等
- 创建这种 PV 需要用到的存储插件，比如 Ceph

有了这样两个信息之后，Kubernetes 就能够根据用户提交的 PVC，找到一个对应的 StorageClass 了。然后，Kubernetes 就会调用该 `StorageClass` 声明的存储插件，创建出需要的 PV。


## 总结

![](https://static001.geekbang.org/resource/image/e8/d9/e8b2586e4e14eb54adf8ff95c5c18cd9.png)
- PVC 描述的，是 pod 想要使用的持久化存储的属性，比如存储大小，读取权限等
- PV 描述的，则是一个具体的 Volume 的属性，比如 Volume 的类型、挂载目录、远程存储服务器地址等。
- 而 StorageClass 的作用，则是充当 PV 的模板。并且，只有**同属于一个 StorageClass 的 PV 和 PVC**，才可以绑定在一起。

当然，StorageClass 的另一个重要作用，**是指定 PV 的 Provisioner（存储插件）**。这时候，如果你的存储插件支持 Dynamic Provisioning 的话，Kubernetes 就可以**自动为你创建** PV 了。
