# kubernetes 网络模型与 CNI 网络插件（需要再次阅读）

在 Flanner 项目中，UDP 和 VXLAN 的实现都一个共性：那就是用户的容器都是连接在docker0 网桥上。而网络插件则在宿主机上创建一个特殊的设备（**UDP创建的是 TUN 设备，VXLAN模式创建的 VTEP 设备**），docker0 与这个设备之间的，通过 IP 转发（路由表）进行协作

**网络插件真正要做的事情，则是通过某种方法，吧不同宿主机上的特殊设备连通，从而达到容器跨主机通信的目的。**

实际上，上面这个流程，也正是 Kubernetes 对容器网络的主要处理方法。只不过，Kubernetes 是通过一个叫作 CNI 的接口，维护了一个单独的网桥来代替 docker0。这个网桥的名字就叫作：**CNI 网桥**，它在宿主机上的设备名称默认是：**cni0**。

以 Flannel 的 VXLAN 模式为例，在 Kubernetes 环境里，它的工作方式跟我们在上一篇文章中讲解的没有任何不同。只不过，docker0 网桥被替换成了 CNI 网桥而已，如下所示：

![](https://static001.geekbang.org/resource/image/7b/21/7b03e1604326b7cf355068754f47e821.png)

CNI 网桥只是接管所有 CNI 插件负责的、即 Kubernetes 创建的容器（Pod）。

Kubernetes 之所以要设置这样一个与 docker0 网桥**功能几乎一样**的 CNI 网桥，主要原因包括两个方面：
- **kubernetes 项目并没有使用 Docker 网络模型(CNM),所以它并不希望、也不具备配置 docker0 网桥的能力；**
- **另一方面，这还与 Kubernetes 如何配置 Pod，也就是 Infra 容器的 Network Namespace 密切相关。**

所以 CNI 的设计思想：**kubernetes 在启动 Infra 容器之后，就可以直接调用 CNI 网络插件，为这个 Infra 容器的 NetWork Namespace ，配置符合预期的网络栈**

## 如何实现网络栈的配置工作

从 CNI 插件的部署和实现方式说起。

我们在部署 Kubernetes 的时候，有一个步骤是安装 kubernetes-cni 包，它的目的就是在宿主机上**安装CNI 插件所需的基础可执行文件**。

在安装完成后，你可以在宿主机的 /opt/cni/bin 目录下看到它们，如下所示：


```
ls -al /opt/cni/bin/
total 73088
-rwxr-xr-x 1 root root  3890407 Aug 17  2017 bridge
-rwxr-xr-x 1 root root  9921982 Aug 17  2017 dhcp
-rwxr-xr-x 1 root root  2814104 Aug 17  2017 flannel
-rwxr-xr-x 1 root root  2991965 Aug 17  2017 host-local
-rwxr-xr-x 1 root root  3475802 Aug 17  2017 ipvlan
-rwxr-xr-x 1 root root  3026388 Aug 17  2017 loopback
-rwxr-xr-x 1 root root  3520724 Aug 17  2017 macvlan
-rwxr-xr-x 1 root root  3470464 Aug 17  2017 portmap
-rwxr-xr-x 1 root root  3877986 Aug 17  2017 ptp
-rwxr-xr-x 1 root root  2605279 Aug 17  2017 sample
-rwxr-xr-x 1 root root  2808402 Aug 17  2017 tuning
-rwxr-xr-x 1 root root  3475750 Aug 17  2017 vlan

```

这些 CNI 的基础可执行文件，按照功能可以分为三类：

**第一类，叫作 Main 插件，它是用来创建具体网络设备的二进制文件。**比如，bridge（网桥设备）、ipvlan、loopback（lo 设备）、macvlan、ptp（Veth Pair 设备），以及 vlan。

我在前面提到过的 Flannel、Weave 等项目，都属于“网桥”类型的 CNI 插件。所以在具体的实现中，它们往往会调用 bridge 这个二进制文件。这个流程，我马上就会详细介绍到。

**第二类，叫作 IPAM（IP Address Management）插件，它是负责分配 IP 地址的二进制文件**。比如，dhcp，这个文件会向 DHCP 服务器发起请求；host-local，则会使用预先配置的 IP 地址段来进行分配。

**第三类，是由 CNI 社区维护的内置 CNI 插件。**比如：flannel，就是专门为 Flannel 项目提供的 CNI 插件；tuning，是一个通过 sysctl 调整网络设备参数的二进制文件；portmap，是一个通过 iptables 配置端口映射的二进制文件；bandwidth，是一个使用 Token Bucket Filter (TBF) 来进行限流的二进制文件。

从这些二进制文件中，我们可以看到，如果要实现一个给 Kubernetes 用的容器网络方案，其实需要做**两部分工作**，以 Flannel 项目为例：

**首先，实现这个网络方案本身**。这一部分需要编写的，其实就是 flanneld 进程里的主要逻辑。比如，创建和配置 flannel.1 设备、配置宿主机路由、配置 ARP 和 FDB 表里的信息等等。

**然后，实现该网络方案对应的 CNI 插件。**这一部分主要需要做的，就是配置 Infra 容器里面的网络栈，并把它连接在 CNI 网桥上。

接下来，你就需要在宿主机上安装 flanneld（网络方案本身）。而在这个过程中，flanneld 启动后会在每台宿主机上生成它**对应的CNI 配置文件**（它其实是一个 **ConfigMap**），从而告诉 Kubernetes，这个集群要使用 Flannel 作为容器网络方案


这个 CNI 配置文件的内容如下所示：


```
cat /etc/cni/net.d/10-flannel.conflist 
{
  "name": "cbr0",
  "plugins": [
    {
      "type": "flannel",
      "delegate": {
        "hairpinMode": true,
        "isDefaultGateway": true
      }
    },
    {
      "type": "portmap",
      "capabilities": {
        "portMappings": true
      }
    }
  ]
}

```

需要注意的是，在 Kubernetes 中，**处理容器网络相关的逻辑**并不会在 `kubelet` 主干代码里执行，而是会在具体的 `CRI`（Container Runtime Interface，容器运行时接口）实现里完成。对于 Docker 项目来说，它的 CRI 实现叫作 `dockershim`，你可以在 kubelet 的代码里找到它。

Kubernetes 目前**不支持多个 CNI 插件混用**。如果你在 CNI 配置目录（/etc/cni/net.d）里放置了多个 CNI 配置文件的话，dockershim 只会加载按字母顺序排序的第一个插件。

但另一方面，CNI 允许你在一个 **CNI 配置文件里，通过 plugins 字段**，定义多个插件进行协作。

比如，在我们上面这个例子里，Flannel 项目就指定了 `flannel` 和 `portmap` 这两个插件。

这时候，dockershim 会把这个 CNI 配置文件加载起来，并且把列表里的第一个插件、也就是 flannel 插件，设置为**默认插件**。而在后面的执行过程中，flannel 和 portmap 插件**会按照定义顺序被调用，从而依次完成“配置容器网络”和“配置端口映射”这两步操作**。

## CNI 插件的工作原理

当 kubelet 组件需要创建 Pod 的时候，它第一个创建的一定是 Infra 容器。所以在这一步，**dockershim 就会先调用 Docker API 创建并启动 Infra 容器**，紧接着执行一个叫作 **SetUpPod** 的方法。这个方法的作用就是：**为 CNI 插件准备参数，然后调用 CNI 插件为 Infra 容器配置网络。**

这里要调用的 CNI 插件，就是 /opt/cni/bin/flannel；而调用它所需要的参数，分为两部分。


- **第一部分：是由 dockershim 设置的一组 CNI 环境变量。**
 其中，最重要的环境变量参数叫作：`CNI_COMMAND`。它的取值只有两种：`ADD` 和 `DEL`。

**这个 ADD 和 DEL 操作，就是 CNI 插件唯一需要实现的两个方法**。

其中 **ADD** 操作的含义是：把容器添加到 CNI 网络里；**DEL** 操作的含义则是：把容器从 CNI 网络里移除掉。

而对于网桥类型的 CNI 插件来说，这两个操作意味着把容器以**Veth Pair 的方式“插”到 CNI 网桥上，或者从网桥上“拔”掉**。


### ADD 详解
CNI 的 ADD 操作需要的参数包括：容器里**网卡的名字**eth0（CNI_IFNAME），Pod 的 Network Namespace **文件的路径**（CNI_NETNS），容器的 ID（CNI_CONTAINERID）等。这些参数都属于上述**环境变量**里的内容。其中，Pod（Infra 容器）的 Network Namespace 文件的路径，我在前面讲解容器基础的时候提到过，即：/proc/< 容器进程的 PID>/ns/net。

除此之外，在 CNI 环境变量里，还有一个叫作 `CNI_ARGS` 的参数。通过这个参数，CRI 实现（比如 `dockershim`）就可以以 `Key-Value` 的格式，**传递自定义信息给网络插件**。这是用户将来自定义 CNI 协议的一个重要方法。

- **第二部分，则是 dockershim 从 CNI 配置文件里加载到的、默认插件的配置信息。**

这个配置信息在 CNI 中被叫作 `Network Configuration`，它的完整定义你可以参考这个文档。dockershim 会把 Network Configuration 以 `JSON` 数据的格式，通过标准输入（stdin）的方式传递给 **Flannel CNI 插件**。

而有了这两部分参数，Flannel CNI 插件实现 ADD 操作的过程就非常简单了。

不过，需要注意的是，Flannel 的 CNI 配置文件（ /etc/cni/net.d/10-flannel.conflist）里有这么一个字段，叫作 `delegate`：


```
...
     "delegate": {
        "hairpinMode": true,
        "isDefaultGateway": true
      }

```

`Delegate` 字段的意思是，这个 CNI 插件并不会自己做事儿，而是会调用 Delegate 指定的某种 CNI 内置插件来完成。对于 Flannel 来说，它**调用**的 `Delegate` 插件，就是前面介绍到的 CNI bridge 插件。

所以说，`dockershim` 对 插件的调用，其实就是走了个过场。Flannel CNI 插件**唯一**需要做的，就是**对 dockershim 传来的 Network Configuration 进行补充**。比如，将 Delegate 的 Type 字段设置为 bridge，将 Delegate 的 IPAM 字段设置为 host-local 等

经过 Flannel CNI 插件补充后的、完整的 Delegate 字段如下所示：


```
{
    "hairpinMode":true,
    "ipMasq":false,
    "ipam":{
        "routes":[
            {
                "dst":"10.244.0.0/16"
            }
        ],
        "subnet":"10.244.1.0/24",
        "type":"host-local"
    },
    "isDefaultGateway":true,
    "isGateway":true,
    "mtu":1410,
    "name":"cbr0",
    "type":"bridge"
}

```
其中，ipam 字段里的信息，比如 10.244.1.0/24，读取自 Flannel 在宿主机上生成的 Flannel 配置文件，即：宿主机上的 /run/flannel/subnet.env 文件。

接下来，Flannel CNI 插件就会调用 CNI bridge 插件，也就是执行：/opt/cni/bin/bridge 二进制文件。

这一次，调用 CNI bridge 插件需要的两部分参数的第一部分、也就是 **CNI 环境变量**，并没有变化。所以，它里面的 CNI_COMMAND 参数的值还是“ADD”。


而第二部分 **Network Configration**，正是上面补充好的 **Delegate** 字段。Flannel CNI 插件会把 `Delegate` 字段的内容以标准输入（stdin）的方式传递给 CNI bridge 插件。

**有了这两部分参数，接下来 CNI bridge 插件就可以“代表”Flannel，进行“将容器加入到 CNI 网络里”这一步操作了。而这一部分内容，与容器 Network Namespace 密切相关，所以我要为你详细讲解一下。**

1. 首先，CNI bridge 插件会在宿主机上检查 CNI 网桥是否存在。如果没有的话，那就创建它。
    ```
    # 在宿主机上
    $ ip link add cni0 type bridge
    $ ip link set cni0 up
    
    ```
2. 接下来，CNI bridge 插件会通过 **Infra** 容器的 Network Namespace 文件，进入到这个 Network Namespace 里面，然后创建一对 **Veth Pair** 设备。
3. 紧接着，它会把这个 Veth Pair 的其中一端，“移动”到宿主机上。这相当于在容器里执行如下所示的命令：
        
    ```
    # 在容器里
    
    # 创建一对 Veth Pair 设备。其中一个叫作 eth0，另一个叫作 vethb4963f3
    $ ip link add eth0 type veth peer name vethb4963f3
    
    # 启动 eth0 设备
    $ ip link set eth0 up 
    
    # 将 Veth Pair 设备的另一端（也就是 vethb4963f3 设备）放到宿主机（也就是 Host Namespace）里
    $ ip link set vethb4963f3 netns $HOST_NS
    
    # 通过 Host Namespace，启动宿主机上的 vethb4963f3 设备
    $ ip netns exec $HOST_NS ip link set vethb4963f3 up 
    
    ```
    
这样，vethb4963f3 就出现在了宿主机上，而且这个 Veth Pair 设备的另一端，就是容器里面的 eth0。
1. 接下来，CNI bridge 插件就可以把 vethb4963f3 设备连接在 CNI 网桥上。

在将 vethb4963f3 设备连接在 CNI 网桥之后，CNI bridge 插件还会为它设置 **Hairpin Mode（发夹模式）**。这是因为，**在默认情况下，网桥设备是不允许一个数据包从一个端口进来后，再从这个端口发出去的**。但是，它允许你为这个端口开启 Hairpin Mode，从而取消这个限制。

这个特性，主要用在容器需要通过NAT（即：端口映射）的方式，**“自己访问自己”**的场景下。



## 总结
CNI 网络的实现原理。根据这个原理，你其实就很容易理解所谓的“Kubernetes 网络模型”了：

1. 所有容器都可以直接使用 IP 地址与其他容器通信，而无需使用 NAT。
2. 所有宿主机都可以直接使用 IP 地址与所有容器通信，而无需使用 NAT。反之亦然。
3. 容器自己“看到”的自己的 IP 地址，和别人（宿主机或者容器）看到的地址是完全一样的。