# HPCCM  教程

## 支持的软件
其支持的的用法和`module commands`很相似

- mlnx_ofed：OpenFabrics
- gnu()：GNU compiler
- openmpi： GNU compiler
- [SCI-F](https://sci-f.github.io):


## MPI Bandwidth
这个例子讲述了如何使用`HPCCM`配方来创建应用程序容器。
```
Stage0 += baseimage(image='centos:7')
Stage0 += gnu(fortran=False)
Stage0 += mlnx_ofed()
Stage0 += openmpi(cuda=False)
```
上述使用了` centos:7`作为基础镜像，`mlnx_ofed`用户软件库，`mpi`库和` gnu`编译器。

下一步是从源代码构建MPI带宽程序。首先，源代码必须被复制到容器中，然后被编译。对于这两个步骤，将使用HPCCM原语。HPCCM原语是本机容器规范操作的包装器，将概念操作转换为相应的本机容器特定语法。原语还隐藏了Docker和`Singularity `容器映像构建过程之间的许多行为差异，这样无论输出配置规范格式如何，行为都是一致的。

首先，将MPI带宽源代码下载到`recipe`所在的目录中。然后源代码的本地副本可以被复制到容器图像中。

```
Stage0 += copy(src='mpi_bandwidth.c', dest='/var/tmp/mpi_bandwidth.c')
```

最后，使用`mpicc MPI`编译器编译程序二进制文件。

```
Stage0 += shell(commands=[
    'mpicc -o /usr/local/bin/mpi_bandwidth /var/tmp/mpi_bandwidth.c']) 
```

上述操作完成的过程如下：

```
# CentOS base image
Stage0 += baseimage(image='centos:7')

# GNU compilers
Stage0 += gnu(fortran=False)

# Mellanox OFED
Stage0 += mlnx_ofed()

# OpenMPI
Stage0 += openmpi(cuda=False)

# MPI Bandwidth
Stage0 += copy(src='mpi_bandwidth.c', dest='/var/tmp/mpi_bandwidth.c')
Stage0 += shell(commands=[
    'mpicc -o /usr/local/bin/mpi_bandwidth /var/tmp/mpi_bandwidth.c'])

```

假设配方文件被命名为mpi_bandwidth.py，以下步骤生成Docker和奇点容器图像，然后演示在单个节点上运行程序。

运行过程：
```go
$ hpccm --recipe mpi_bandwidth.py --format docker > Dockerfile
$ sudo docker build -t mpi_bandwidth -f Dockerfile .
$ sudo docker run --rm -it mpirun --allow-run-as-root -n 2 /usr/local/bin/mpi_bandwidth 
```
## 用法
在` recipe.py`中添加好软件基本的依赖之后，用户根据预期的工作流程，接下的可能是使用文本编辑器将构建高性能计算应用程序的步骤添加到`dockerfile`或`Singularity`定义文件中，或者可能是扩展HPCCM recipe 以添加构建高性能计算应用程序的步骤。

## 多阶段构建
多阶段构建是一种非常有用的功能，它将应用程序构建步骤与部署步骤分开。当在容器中部署构建的应用程序时，开发工具链、应用程序源代码和构建工件不是必需的。事实上，它们会显著且不必要地增加容器图像的大小。

`hpccm`命令行工具自动创建两个阶段，阶段0和阶段1。大多数构建块提供了一个运行时方法来安装组件的相应运行时版本。

以下方法在第一阶段(构建)构建`OpenMPI`，然后将生成的`OpenMPI`构建复制到第二阶段(部署)。第一阶段中定义的构件设置会自动反映在第二阶段中。

```python
Stage0 += baseimage(image='nvidia/cuda:9.0-devel-centos7')
ompi = openmpi(infiniband=False, prefix='/opt/openmpi')
Stage0 += ompi

Stage1 += baseimage(image='nvidia/cuda:9.0-base-centos7')
Stage1 += ompi.runtime()
```

MILC的示例配方展示了多阶段配方的实用性。从第一阶段构建的Docker容器映像仅为5.93 GB，而当采用多阶段构建过程时，容器映像仅为429 MB。

```
$ wget https://raw.githubusercontent.com/NVIDIA/hpc-container-maker/master/recipes/milc/milc.py
$ hpccm --recipe milc.py --single-stage > Dockerfile.single-stage
$ sudo docker build -t milc:single-stage -f Dockerfile.single-stage .

$ hpccm --recipe milc.py > Dockerfile.multi-stage
$ sudo docker build -t milc:multi-stage -f Dockerfile.multi-stage .

$ docker images --format "{{.Repository}}:{{.Tag}}: {{.Size}}" milc
milc:multi-stage: 429MB
milc:single-stage: 5.93GB

```

奇点3.2版及更高版本支持多阶段奇点定义文件。但是，多阶段定义文件语法与奇点的早期版本不兼容。使用`HPCCM` `--singularity-version <version>`命令行选项指定要生成的奇点定义文件版本。3.2或更高版本将生成一个多阶段定义文件，该文件将仅使用奇点3.2或更高版本构建。低于3.2的版本将生成一个可移植的定义文件，该文件适用于奇点的任何版本，但不支持多阶段构建。

## Scientific Filesystem (SCI-F)
`Scientific Filesystem (SCI-F)`提供容器的内部模块化。例如，单个容器可能需要包含应用程序工作负载的多个版本，每个版本都针对特定的硬件配置进行了调整，以实现尽可能广泛的部署。

scif构建模块提供了一个与SCI-F的接口，在语法上类似于阶段。可以使用+=语法将其他构建块或原语添加到SCI-F配方中。

为了帮助理解在同一个容器中包含多个应用程序二进制文件在哪里是有用的，考虑一下GPU的计算能力。GPU的计算能力指定了它的可用特性。GPU无法运行为更高计算能力编译的代码，但是许多优化和其他高级功能只有在更高的计算能力下才可用。一般来说，GPU可以运行以较低计算能力构建的代码，但这意味着不能利用最近的GPU的某些功能.

解决这种紧张关系的一种方法是构建二进制文件的多个版本，每个版本用于特定的计算能力，并在运行容器时根据可用硬件选择最佳版本。(更好的方法是构建一个单一的“胖”二进制文件，或者使用PTX来实现即时编译，但是一些应用程序构建系统可能不支持这些技术。)

以下方法为三种不同的CUDA计算能力构建CUDA-STREAM

```
Stage0 += baseimage(image='nvidia/cuda:9.1-devel-centos7')

# Install the GNU compiler
Stage0 += gnu(fortran=False)

# Install SCI-F
Stage0 += pip(packages=['scif'])

# Download a single copy of the source code
Stage0 += packages(ospackages=['ca-certificates', 'git'])
Stage0 += shell(commands=['cd /var/tmp',
                          'git clone --depth=1 https://github.com/bcumming/cuda-stream.git cuda-stream'])

# Build CUDA-STREAM as a SCI-F application for each CUDA compute capability
for cc in ['35', '60', '70']:
  binpath = '/scif/apps/cc{}/bin'.format(cc)

  stream = scif(name='cc{}'.format(cc))
  stream += comment('CUDA-STREAM built for CUDA compute capability {}'.format(cc))
  stream += shell(commands=['nvcc -std=c++11 -ccbin=g++ -gencode arch=compute_{0},code=\\"sm_{0},compute_{0}\\" -o {1}/stream /var/tmp/cuda-stream/stream.cu'.format(cc, binpath)])
  stream += environment(variables={'PATH': '{}:$PATH'.format(binpath)})
  stream += label(metadata={'COMPUTE_CAPABILITY': cc})
  stream += runscript(commands=['stream'])

  Stage0 += stream
```
为该配方生成文档文件时，HPCCM还将在当前目录中创建3个SCI-F配方文件

```
$ hpccm --recipe scif.py --format docker > Dockerfile
$ sudo docker build -t cuda-stream -f Dockerfile .
```

奇点(Singularity)对SCI-F有本机支持，在这种情况下没有必要安装scif PyPi包，但这样做也可以。

```
$ hpccm --recipe scif.py --format singularity > Singularity.def
$ sudo singularity build cuda-stream.simg Singularity.def
```

可以使用- app命令行选项选择SCI-F应用程序。
```
$ singularity apps cuda-stream.simg
cc35
cc60
cc70
$ singularity run --nv --app cc60 cuda-stream.simg
 STREAM Benchmark implementation in CUDA
 ...
```

如果安装了scif PyPi包(奇点可选)，那么scif程序也可以用于等效功能。

```
$ singularity run cuda-stream.simg scif apps
...
$ singularity run --nv cuda-stream.simg scif run cc60
...

```

HPCCM scif模块不限于应用。构件也可以安装在SCI-F 应用中。下面将在容器中安装两个版本的OpenMPI，一个用InfiniBand谓词构建，另一个用UCX构建，并且为每个版本构建MPI带宽程序。


```
# CentOS base image
Stage0 += baseimage(image='nvidia/cuda:9.1-devel-centos7')

# GNU compilers
Stage0 += gnu(fortran=False)

# Mellanox OFED
Stage0 += mlnx_ofed()

# SCI-F
Stage0 += pip(packages=['scif'])

# Download MPI Bandwidth source code
Stage0 += shell(commands=[
    'wget --user-agent "" -q -nc --no-check-certificate -P /var/tmp https://computing.llnl.gov/tutorials/mpi/samples/C/mpi_bandwidth.c'])

# OpenMPI 3.1 w/ InfiniBand verbs
ompi31 = scif(name='ompi-3.1-ibverbs')
ompi31 += comment('MPI Bandwidth built with OpenMPI 3.1 using InfiniBand verbs')
ompi31 += openmpi(infiniband=True, prefix='/scif/apps/ompi-3.1-ibverbs',
                  ucx=False, version='3.1.3')
ompi31 += shell(commands=['cd /scif/apps/ompi-3.1-ibverbs/bin',
                          './mpicc -o mpi_bandwidth /var/tmp/mpi_bandwidth.c'])
Stage0 += ompi31

# OpenMPI 4.0 w/ UCX
ompi40 = scif(name='ompi-4.0-ucx')
ompi40 += comment('MPI Bandwidth built with OpenMPI 4.0 using UCX')
ompi40 += gdrcopy()
ompi40 += knem()
ompi40 += xpmem()
ompi40 += ucx(knem='/usr/local/knem')
ompi40 += openmpi(infiniband=False, prefix='/scif/apps/ompi-4.0-ucx',
                  ucx='/usr/local/ucx', version='4.0.0')
ompi40 += shell(commands=['cd /scif/apps/ompi-4.0-ucx/bin',
                          './mpicc -o mpi_bandwidth /var/tmp/mpi_bandwidth.c'])
Stage0 += ompi40
```
一个更复杂的容器可能包括一个入口点，该入口点检测硬件配置并自动使用最合适的SCI-F应用程序环境。


这个文档，基本上解释了该如何构建一个 hpc 环境中运行的镜像。